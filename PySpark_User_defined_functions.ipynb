{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark User defined functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH3g2EqsUw2a46AZbRUPCo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravSahani1417/PySpark-Basic-Implementation/blob/main/PySpark_User_defined_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-C4nzTjKCP_",
        "outputId": "d2374f01-a2a6-4444-f6c2-c90db131b9b5"
      },
      "source": [
        "# Setting up the PySpark environment\n",
        "\n",
        "# Install java 8\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Apache Spark binary: This link can change based on the version. Update this link with the latest version before using\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Unzip file\n",
        "!tar -xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "\n",
        "# Install findspark: Adds Pyspark to sys.path at runtime\n",
        "!pip install -q findspark\n",
        "\n",
        "# Install pyspark\n",
        "!pip install pyspark\n",
        "\n",
        "# Add environmental variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "\n",
        "# findspark will locate spark in the system\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [1 InRelease 14.2 kB/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connected to cloud.r-pro\r0% [1 InRelease gpgv 88.7 kB] [Waiting for headers] [Waiting for headers] [Wait\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,415 kB]\n",
            "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [450 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,185 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:17 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [61.8 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [800 kB]\n",
            "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,772 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [481 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,619 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [906 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,185 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.5 kB]\n",
            "Get:27 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.6 kB]\n",
            "Fetched 13.3 MB in 4s (3,785 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 71kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 19.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=72948dfe16f0832d502e75378bf5bd8988a7368555499725080df15e43948d2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYAyrwE-LcrD"
      },
      "source": [
        "#Spark Initialization\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local\") \\\n",
        "        .appName(\"Hands-on PySpark on Google Colab\") \\\n",
        "        .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "zelSZZIeLhO9",
        "outputId": "7f2e0a04-220f-46cc-8896-15c3325f323d"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a6f16c568290:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Hands-on PySpark on Google Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7efd739f9390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKVkkcsiLju4"
      },
      "source": [
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/00603/in-vehicle-coupon-recommendation.csv -P sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS1SGz10LpUw",
        "outputId": "ae722a84-2a13-4ca2-b829-c48ca1a64b84"
      },
      "source": [
        "# We can set header='true' and inferSchema='true' to infer the schema while reading the data\n",
        "\n",
        "filepath = \"sample_data/in-vehicle-coupon-recommendation.csv\"\n",
        "spark_df = spark.read.format('csv').options(header='true', inferSchema='true').load(filepath)\n",
        "spark_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "|destination    |passanger|weather|temperature|time|coupon               |expiration|gender|age|maritalStatus    |has_children|education               |occupation|income         |car |Bar  |CoffeeHouse|CarryAway|RestaurantLessThan20|Restaurant20To50|toCoupon_GEQ5min|toCoupon_GEQ15min|toCoupon_GEQ25min|direction_same|direction_opp|Y  |\n",
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "|No Urgent Place|Alone    |Sunny  |55         |2PM |Restaurant(<20)      |1d        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |0                |0                |0             |1            |1  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |10AM|Coffee House         |2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |0                |0                |0             |1            |0  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |10AM|Carry out & Take away|2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |1  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |2PM |Coffee House         |2h        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |0  |\n",
            "|No Urgent Place|Friend(s)|Sunny  |80         |2PM |Coffee House         |1d        |Female|21 |Unmarried partner|1           |Some college - no degree|Unemployed|$37500 - $49999|null|never|never      |null     |4~8                 |1~3             |1               |1                |0                |0             |1            |0  |\n",
            "+---------------+---------+-------+-----------+----+---------------------+----------+------+---+-----------------+------------+------------------------+----------+---------------+----+-----+-----------+---------+--------------------+----------------+----------------+-----------------+-----------------+--------------+-------------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZNb71ChMHPZ"
      },
      "source": [
        "Introduction to UDF: User Defined Functions\n",
        "\n",
        "It extends the capability of Spark by defining our own functions in Python. \n",
        "\n",
        "*   It extends the capability of Spark by defining our own functions in Python. \n",
        "*   PySpark doesn't provide all the possible transformations. For example: you want to select middle letter in a string column. You cannot do it straight away using PySpark built-in functions. In this case, you can write a udf python function and use it to transform a PySpark DataFrame.\n",
        "\n",
        "\n",
        "*   If the UDF is not properly created, the you might face performance issues. So, you have to create the function in a optimal way.\n",
        "*   NOTE: My suggestion is to use udf functions, if there are no in-built Spark SQL functions to achieve that task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGq0Dx1VMhOC"
      },
      "source": [
        "Steps for using UDF:\n",
        "\n",
        "*   Create a python function\n",
        "*   Convert the above python function to UDF\n",
        "*   Use this UDF function on a PySpark column for transforming data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMSvedchL8uT"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sCEVBcvMx91",
        "outputId": "2c7d946b-eb47-4270-9fe2-5c7694781514"
      },
      "source": [
        "columns_to_use = [\"destination\", \"passanger\", \"weather\", \"time\", \"coupon\", \"income\"]\n",
        "spark_df = spark_df.select(*columns_to_use)\n",
        "spark_df.show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "|destination    |passanger|weather|time|coupon               |income         |\n",
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "|No Urgent Place|Alone    |Sunny  |2PM |Restaurant(<20)      |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Coffee House         |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Carry out & Take away|$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$37500 - $49999|\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$37500 - $49999|\n",
            "+---------------+---------+-------+----+---------------------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es3nrW7QM4Pc",
        "outputId": "06351262-765a-4b6b-e80a-bbd8d945848c"
      },
      "source": [
        "#Goal: Convert income column to numerical column by taking the average of income range\n",
        "\n",
        "spark_df.select(\"income\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------+\n",
            "|          income|\n",
            "+----------------+\n",
            "| $75000 - $87499|\n",
            "| $12500 - $24999|\n",
            "|Less than $12500|\n",
            "| $50000 - $62499|\n",
            "| $25000 - $37499|\n",
            "| $37500 - $49999|\n",
            "| $62500 - $74999|\n",
            "| $87500 - $99999|\n",
            "| $100000 or More|\n",
            "+----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlbxkGj7N1s3"
      },
      "source": [
        "## STEP 1: Create a python function\n",
        "\n",
        "def transform_income(income_str):\n",
        "\n",
        "    income_str = str(income_str)\n",
        "\n",
        "    if income_str[0] == \"L\":\n",
        "        income_str = income_str.split(\" \")[-1]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    elif income_str[-1] == \"e\":\n",
        "        income_str = income_str.split(\" \")[0]\n",
        "        avg_income = income_str[1:]\n",
        "        avg_income = float(avg_income)\n",
        "        return avg_income\n",
        "\n",
        "    else:\n",
        "        income_str = income_str.split(\" - \")\n",
        "        avg_income = (int(income_str[0][1:]) + int(income_str[1][1:]))/2\n",
        "        return avg_income"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2k9XEGVODz-",
        "outputId": "5d91891e-f613-4314-b467-5b1d7701c766"
      },
      "source": [
        "transform_income(\"Less than $12500\"), transform_income(\"$100000 or More\"), transform_income(\"$37500 - $49999\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12500.0, 100000.0, 43749.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yiv3StMLOJIo"
      },
      "source": [
        "## STEP 2: Convert python function to UDF function\n",
        "\n",
        "transform_income_udf = F.udf(f=lambda row: transform_income(row), returnType=T.FloatType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnqUn3EGONnw",
        "outputId": "6feecb05-c137-4a9f-850b-f8aee7fa6172"
      },
      "source": [
        "## STEP 3: Apply the udf function\n",
        "\n",
        "updated_spark_df = spark_df.withColumn(\"income_float\", transform_income_udf(F.col(\"income\")))\n",
        "updated_spark_df.sample(0.2).show(10, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "|destination    |passanger|weather|time|coupon               |income         |income_float|\n",
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "|No Urgent Place|Friend(s)|Sunny  |10AM|Carry out & Take away|$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Kid(s)   |Sunny  |2PM |Restaurant(<20)      |$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Kid(s)   |Sunny  |2PM |Restaurant(<20)      |$37500 - $49999|43749.5     |\n",
            "|No Urgent Place|Alone    |Sunny  |2PM |Restaurant(<20)      |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |2PM |Coffee House         |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Friend(s)|Sunny  |6PM |Restaurant(<20)      |$62500 - $74999|68749.5     |\n",
            "|No Urgent Place|Alone    |Sunny  |10AM|Coffee House         |$62500 - $74999|68749.5     |\n",
            "|Home           |Alone    |Sunny  |6PM |Bar                  |$62500 - $74999|68749.5     |\n",
            "+---------------+---------+-------+----+---------------------+---------------+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sut8zIJCOT4j",
        "outputId": "b0788c2f-ddac-48d5-988a-d56b35030255"
      },
      "source": [
        "updated_spark_df.filter(F.col(\"income\") == \"Less than $12500\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "|    destination|passanger|weather|time|         coupon|          income|income_float|\n",
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "|No Urgent Place|    Alone|  Sunny| 2PM|Restaurant(<20)|Less than $12500|     12500.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|   Coffee House|Less than $12500|     12500.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|            Bar|Less than $12500|     12500.0|\n",
            "+---------------+---------+-------+----+---------------+----------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DO7dGmnJOZu7",
        "outputId": "6a26b770-3611-4549-e9c3-af6679da2e6f"
      },
      "source": [
        "updated_spark_df.filter(F.col(\"income\") == \"$100000 or More\").show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "|    destination|passanger|weather|time|         coupon|         income|income_float|\n",
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "|No Urgent Place|    Alone|  Sunny| 2PM|Restaurant(<20)|$100000 or More|    100000.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|   Coffee House|$100000 or More|    100000.0|\n",
            "|No Urgent Place|Friend(s)|  Sunny|10AM|            Bar|$100000 or More|    100000.0|\n",
            "+---------------+---------+-------+----+---------------+---------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obcfw4E_Och-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}